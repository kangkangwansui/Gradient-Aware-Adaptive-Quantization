Gradient Aware Adaptive Quantization (GAAQ)
This repository is dedicated to the code associated with the research paper titled "Gradient Aware Adaptive Quantization: Locally Uniform Quantization with Learnable Clipping Thresholds for Globally Non-Uniform Weights." The paper explores an innovative approach to neural network quantization that leverages gradient information to adaptively set clipping thresholds, enabling more efficient and accurate quantization.

About the Paper
Our paper introduces a novel non-uniform quantization method that meticulously accounts for the distribution of weights throughout the quantization process. More significantly, it integrates the gradient information of the weights, thereby enabling a more sophisticated and effective quantization strategy. This method is designed to balance the trade-off between model efficiency and accuracy retention, particularly for convolutional neural networks (CNNs).

Repository Contents
At this stage, the repository is under construction. The paper is currently under review, and once it is accepted, we will make the code publicly available here. The code will include:

Implementation of the GAAQ algorithm.
Pre-trained models used for experimentation.
Scripts for reproducing the results presented in the paper.
Documentation and usage instructions.
Future Updates
We are excited to share our work with the community and look forward to your feedback and contributions once the code is released. Please watch this repository for updates, and feel free to reach out with any questions or inquiries in the meantime.

Contact
For more information, please contact the authors directly:

[Zhou Kang] (2427654570@qq.com)
